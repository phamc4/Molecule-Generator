{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3')\n",
    "\n",
    "bucket = 'smilesmolecules'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_files(client, bucket):\n",
    "    length = 0\n",
    "    csv_files = []\n",
    "    content = client.list_objects(Bucket=bucket).get('Contents')\n",
    "    for obj in content:\n",
    "        if length > 50:\n",
    "            break\n",
    "        key = obj.get('Key')\n",
    "        \n",
    "        if '.smi' in key:\n",
    "                csv_files.append(key)\n",
    "                length += 1 \n",
    "    return csv_files\n",
    "\n",
    "test = get_files(client, bucket)\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The whole S3 bucket contains more than 900 million molecules. We'll grab around 10 million as a starter.\n",
    "'''\n",
    "\n",
    "df = []\n",
    "\n",
    "split1 = test[:50]\n",
    "\n",
    "for obj in split1:\n",
    "    dataframe = pd.read_csv('s3://smilesmolecules/' + obj, delimiter = ' ')\n",
    "    df.append(dataframe)\n",
    "\n",
    "    \n",
    "df1 = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.iloc[:1000000, 0]\n",
    "molecules = df1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of unique characters in the molecules set:', 34)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Need to map each character to an int\n",
    "'''\n",
    "unique_chars = sorted(list(set(''.join(molecules[:500000]))))\n",
    "print('Number of unique characters in the molecules set:', len(unique_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = {c: i for i, c in enumerate(unique_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''.join(molecules[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([])\n",
    "y = np.array([])\n",
    "for i in range(0, len(text) - 300):\n",
    "    seq_X = text[i:i + 300]\n",
    "    seq_y = text[i + 300]\n",
    "    X = np.append(X, [char_to_int[char] for char in seq_X])\n",
    "    y = np.append(y, [char_to_int[char] for char in seq_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., 16., 21., ..., 26.,  6., 21.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18902, 300, 1)\n",
      "(18902, 34)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(X, (-1, 300, 1))\n",
    "\n",
    "X = X / len(unique_chars)\n",
    "\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create the LSTM Model'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences= True))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(.25))\n",
    "\n",
    "model.add(LSTM(512, return_sequences = True))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(LSTM(256, return_sequences = True))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(y.shape[1], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 300, 128)          66560     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 300, 256)          394240    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 300, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 300, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 300, 256)          787456    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 34)                4386      \n",
      "=================================================================\n",
      "Total params: 3,024,674\n",
      "Trainable params: 3,024,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "18902/18902 [==============================] - 2331s 123ms/step - loss: 2.6908\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.69083, saving model to improved-weights-01-2.6908.hdf5\n",
      "Epoch 2/6\n",
      "18902/18902 [==============================] - 2316s 123ms/step - loss: 2.5768\n",
      "\n",
      "Epoch 00002: loss improved from 2.69083 to 2.57679, saving model to improved-weights-02-2.5768.hdf5\n",
      "Epoch 3/6\n",
      "18902/18902 [==============================] - 2312s 122ms/step - loss: 2.5643\n",
      "\n",
      "Epoch 00003: loss improved from 2.57679 to 2.56428, saving model to improved-weights-03-2.5643.hdf5\n",
      "Epoch 4/6\n",
      "18902/18902 [==============================] - 2311s 122ms/step - loss: 2.5508\n",
      "\n",
      "Epoch 00004: loss improved from 2.56428 to 2.55076, saving model to improved-weights-04-2.5508.hdf5\n",
      "Epoch 5/6\n",
      "18902/18902 [==============================] - 2310s 122ms/step - loss: 2.5474\n",
      "\n",
      "Epoch 00005: loss improved from 2.55076 to 2.54737, saving model to improved-weights-05-2.5474.hdf5\n",
      "Epoch 6/6\n",
      " 6144/18902 [========>.....................] - ETA: 25:54 - loss: 2.5438"
     ]
    }
   ],
   "source": [
    "#Checkpoint\n",
    "filepath = 'improved-weights-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only= True, mode = 'min')\n",
    "callback = [checkpoint]\n",
    "\n",
    "#Fitting\n",
    "model.fit(X, y, epochs = 6, batch_size= 512, callbacks = callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p27",
   "language": "python",
   "name": "conda_amazonei_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
